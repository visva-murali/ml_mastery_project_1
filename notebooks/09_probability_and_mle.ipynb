{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# 09 \u2014 Probability Review + Maximum Likelihood Estimation\n\n**Goal:** Derive MLE for Normal (\u03bc, \u03c3\u00b2) and show OLS \u2261 MLE under Gaussian noise.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "import numpy as np\n\n# PART A: MLE for Normal\n# TODO: Derive by hand (pen+paper), then verify with code below.\nnp.random.seed(0)\nx = np.random.randn(2000) * 2.5 + 3.0  # true mu=3, sigma=2.5\n\ndef mle_normal(x):\n    mu_hat = float(np.mean(x))\n    sigma2_hat = float(np.mean((x - mu_hat)**2))  # MLE uses 1/n, not 1/(n-1)\n    return mu_hat, sigma2_hat\n\nmu_hat, sigma2_hat = mle_normal(x)\nprint(\"\u03bc_hat:\", mu_hat, \"\u03c3\u00b2_hat:\", sigma2_hat)\n\n# PART B: OLS as MLE with Gaussian noise\n# y = X\u03b2 + \u03b5, \u03b5 ~ N(0, \u03c3\u00b2 I)\n# Show that maximizing likelihood == minimizing MSE.\n# TODO: Write a short explanation (markdown cell) and then verify numerically below.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "import warnings\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.datasets import fetch_california_housing, make_regression\nfrom sklearn.model_selection import train_test_split\n\ndef load_regression_data(random_state=42):\n    \"\"\"Return (X, y, feature_names) as numpy arrays.\n    Try California Housing; fallback to synthetic if unavailable (e.g., offline).\n    \"\"\"\n    try:\n        cali = fetch_california_housing(as_frame=True)\n        df = cali.frame.copy()\n        X = df.drop(columns=[\"MedHouseVal\"]).values\n        y = df[\"MedHouseVal\"].values\n        feature_names = list(df.drop(columns=[\"MedHouseVal\"]).columns)\n    except Exception as e:\n        warnings.warn(f\"California Housing fetch failed: {e}. Falling back to synthetic make_regression.\")\n        X, y = make_regression(n_samples=5000, n_features=8, n_informative=6, noise=8.5, random_state=random_state)\n        feature_names = [f\"x{i}\" for i in range(X.shape[1])]\n    return X, y, feature_names\n\ndef train_val_test_split(X, y, random_state=42):\n    # 60/20/20 split: train/val/test\n    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=random_state)\n    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=random_state)\n    return (X_train, y_train), (X_val, y_val), (X_test, y_test)\n\ndef rmse(y_true, y_pred):\n    return float(np.sqrt(np.mean((y_true - y_pred)**2)))\n\ndef mae(y_true, y_pred):\n    return float(np.mean(np.abs(y_true - y_pred)))\n\ndef r2(y_true, y_pred):\n    ss_res = np.sum((y_true - y_pred)**2)\n    ss_tot = np.sum((y_true - np.mean(y_true))**2)\n    return float(1 - ss_res/ss_tot)\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Numerical check: OLS vs maximizing Gaussian log-lik differ by constants -> same beta.\nimport numpy as np\nfrom numpy.linalg import pinv\n\nX, y, feature_names = load_regression_data()\n(X_train, y_train), (X_val, y_val), (X_test, y_test) = train_val_test_split(X, y)\n\nXtr_i = np.c_[np.ones((X_train.shape[0], 1)), X_train]\nbeta_hat = pinv(Xtr_i) @ y_train\n\n# TODO: Implement a function that evaluates Gaussian log-likelihood for a given beta and sigma^2 (use MLE sigma^2).\ndef gaussian_loglik(Xi, y, beta):\n    n = Xi.shape[0]\n    resid = y - Xi @ beta\n    sigma2 = np.mean(resid**2)  # MLE of \u03c3\u00b2 given beta\n    # log-lik up to additive constant: - (n/2) * log(sigma^2) - (1/(2*sigma^2)) * sum(resid^2)\n    return -0.5*n*np.log(sigma2) - 0.5*np.sum(resid**2)/sigma2\n\nll_at_beta_hat = gaussian_loglik(Xtr_i, y_train, beta_hat)\nprint(\"Log-likelihood at OLS beta:\", ll_at_beta_hat)\n\n# TODO: Randomly sample other betas and show none has higher log-lik than OLS within tolerance.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "**Write-up (required):**  \nExplain in your own words why OLS is the MLE under Gaussian i.i.d. noise, and when that assumption breaks.\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}