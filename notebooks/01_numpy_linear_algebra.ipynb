{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# 01 \u2014 NumPy + Linear Algebra for ML\n\n**Goal:** Practice the linear algebra you'll actually use for regression: shapes, dot products, norms, gradients, pseudo-inverse.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "import numpy as np\nfrom numpy.linalg import norm, pinv, svd\n\n# Shapes sanity\nA = np.random.randn(5, 3)\nb = np.random.randn(5)\n\nprint(\"A shape:\", A.shape, \"b shape:\", b.shape)\nprint(\"A^T A shape:\", (A.T @ A).shape)\nprint(\"A^T b shape:\", (A.T @ b).shape)\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# TODO: Implement L2 norm, dot, and a function that computes the projection of b onto the column space of A.\ndef l2(x):\n    return float(np.sqrt(np.sum(x**2)))\n\ndef projection_onto_colspace(A, b):\n    # Use pseudo-inverse: A @ (A^+ b)\n    return A @ (pinv(A) @ b)\n\n# Quick check:\nb_proj = projection_onto_colspace(A, b)\nresidual = b - b_proj\nprint(\"||residual||_2:\", l2(residual))\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# TODO: Derive the gradient of MSE for a linear model y_hat = X @ beta.\n# Then write a function grad_beta(X, y, beta). Verify against finite differences on a toy example.\ndef mse_grad(X, y, beta):\n    n = X.shape[0]\n    return (2.0/n) * (X.T @ (X @ beta - y))\n\n# Finite diff check (small random example)\nnp.random.seed(0)\nX_chk = np.random.randn(20, 4)\nbeta_chk = np.random.randn(4)\ny_chk = X_chk @ beta_chk + 0.1*np.random.randn(20)\n\ng = mse_grad(X_chk, y_chk, beta_chk)\n\n# Finite differences\neps = 1e-6\ng_fd = np.zeros_like(beta_chk)\nfor j in range(len(beta_chk)):\n    e = np.zeros_like(beta_chk); e[j] = eps\n    loss_plus = np.mean((X_chk @ (beta_chk + e) - y_chk)**2)\n    loss_minus = np.mean((X_chk @ (beta_chk - e) - y_chk)**2)\n    g_fd[j] = (loss_plus - loss_minus) / (2*eps)\n\nprint(\"Grad diff ||g - g_fd||:\", np.linalg.norm(g - g_fd))\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}