{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# 06 \u2014 Regularization: Ridge, Lasso, Elastic Net\n\n**Goal:** Understand shrinkage vs sparsity; tune \u03bb; compare coefficient paths.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "import warnings\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.datasets import fetch_california_housing, make_regression\nfrom sklearn.model_selection import train_test_split\n\ndef load_regression_data(random_state=42):\n    \"\"\"Return (X, y, feature_names) as numpy arrays.\n    Try California Housing; fallback to synthetic if unavailable (e.g., offline).\n    \"\"\"\n    try:\n        cali = fetch_california_housing(as_frame=True)\n        df = cali.frame.copy()\n        X = df.drop(columns=[\"MedHouseVal\"]).values\n        y = df[\"MedHouseVal\"].values\n        feature_names = list(df.drop(columns=[\"MedHouseVal\"]).columns)\n    except Exception as e:\n        warnings.warn(f\"California Housing fetch failed: {e}. Falling back to synthetic make_regression.\")\n        X, y = make_regression(n_samples=5000, n_features=8, n_informative=6, noise=8.5, random_state=random_state)\n        feature_names = [f\"x{i}\" for i in range(X.shape[1])]\n    return X, y, feature_names\n\ndef train_val_test_split(X, y, random_state=42):\n    # 60/20/20 split: train/val/test\n    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=random_state)\n    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=random_state)\n    return (X_train, y_train), (X_val, y_val), (X_test, y_test)\n\ndef rmse(y_true, y_pred):\n    return float(np.sqrt(np.mean((y_true - y_pred)**2)))\n\ndef mae(y_true, y_pred):\n    return float(np.mean(np.abs(y_true - y_pred)))\n\ndef r2(y_true, y_pred):\n    ss_res = np.sum((y_true - y_pred)**2)\n    ss_tot = np.sum((y_true - np.mean(y_true))**2)\n    return float(1 - ss_res/ss_tot)\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import Ridge, Lasso, ElasticNet\nfrom sklearn.model_selection import KFold, cross_val_score\n\nX, y, feature_names = load_regression_data()\n(X_train, y_train), (X_val, y_val), (X_test, y_test) = train_val_test_split(X, y)\n\nalphas = np.logspace(-3, 3, 13)\n\ndef cv_rmse(model, X, y):\n    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n    scores = cross_val_score(model, X, y, scoring=\"neg_root_mean_squared_error\", cv=cv, n_jobs=-1)\n    return -scores\n\nridge_rmse = []\nlasso_rmse = []\nenet_rmse = []\n\nfor a in alphas:\n    ridge = Pipeline([(\"scaler\", StandardScaler()), (\"model\", Ridge(alpha=a))])\n    lasso = Pipeline([(\"scaler\", StandardScaler()), (\"model\", Lasso(alpha=a, max_iter=5000))])\n    enet  = Pipeline([(\"scaler\", StandardScaler()), (\"model\", ElasticNet(alpha=a, l1_ratio=0.5, max_iter=5000))])\n    ridge_rmse.append(cv_rmse(ridge, X_train, y_train).mean())\n    lasso_rmse.append(cv_rmse(lasso, X_train, y_train).mean())\n    enet_rmse.append(cv_rmse(enet, X_train, y_train).mean())\n\nplt.figure()\nplt.semilogx(alphas, ridge_rmse, marker=\"o\")\nplt.title(\"Ridge: alpha vs CV RMSE\")\nplt.xlabel(\"alpha\"); plt.ylabel(\"RMSE\")\n\nplt.figure()\nplt.semilogx(alphas, lasso_rmse, marker=\"o\")\nplt.title(\"Lasso: alpha vs CV RMSE\")\nplt.xlabel(\"alpha\"); plt.ylabel(\"RMSE\")\n\nplt.figure()\nplt.semilogx(alphas, enet_rmse, marker=\"o\")\nplt.title(\"ElasticNet: alpha vs CV RMSE (l1_ratio=0.5)\")\nplt.xlabel(\"alpha\"); plt.ylabel(\"RMSE\")\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# TODO: Fit best Ridge and Lasso and plot sorted absolute coefficients side by side; discuss sparsity vs shrinkage.\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import Ridge, Lasso\n\nscaler = StandardScaler().fit(X_train)\nXtr_s = scaler.transform(X_train)\nXval_s = scaler.transform(X_val)\n\nbest_alpha_ridge = float(alphas[np.argmin(ridge_rmse)])\nbest_alpha_lasso = float(alphas[np.argmin(lasso_rmse)])\n\nridge_best = Ridge(alpha=best_alpha_ridge).fit(Xtr_s, y_train)\nlasso_best = Lasso(alpha=best_alpha_lasso, max_iter=5000).fit(Xtr_s, y_train)\n\ncoef_ridge = np.abs(ridge_best.coef_)\ncoef_lasso = np.abs(lasso_best.coef_)\n\nprint(\"Best alpha (Ridge):\", best_alpha_ridge, \"Best alpha (Lasso):\", best_alpha_lasso)\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}