{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# 03 \u2014 Gradient Descent (GD) vs Stochastic Gradient Descent (SGD)\n\n**Goal:** Implement GD and mini-batch SGD for OLS, explore learning rates and convergence.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "import warnings\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.datasets import fetch_california_housing, make_regression\nfrom sklearn.model_selection import train_test_split\n\ndef load_regression_data(random_state=42):\n    \"\"\"Return (X, y, feature_names) as numpy arrays.\n    Try California Housing; fallback to synthetic if unavailable (e.g., offline).\n    \"\"\"\n    try:\n        cali = fetch_california_housing(as_frame=True)\n        df = cali.frame.copy()\n        X = df.drop(columns=[\"MedHouseVal\"]).values\n        y = df[\"MedHouseVal\"].values\n        feature_names = list(df.drop(columns=[\"MedHouseVal\"]).columns)\n    except Exception as e:\n        warnings.warn(f\"California Housing fetch failed: {e}. Falling back to synthetic make_regression.\")\n        X, y = make_regression(n_samples=5000, n_features=8, n_informative=6, noise=8.5, random_state=random_state)\n        feature_names = [f\"x{i}\" for i in range(X.shape[1])]\n    return X, y, feature_names\n\ndef train_val_test_split(X, y, random_state=42):\n    # 60/20/20 split: train/val/test\n    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=random_state)\n    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=random_state)\n    return (X_train, y_train), (X_val, y_val), (X_test, y_test)\n\ndef rmse(y_true, y_pred):\n    return float(np.sqrt(np.mean((y_true - y_pred)**2)))\n\ndef mae(y_true, y_pred):\n    return float(np.mean(np.abs(y_true - y_pred)))\n\ndef r2(y_true, y_pred):\n    ss_res = np.sum((y_true - y_pred)**2)\n    ss_tot = np.sum((y_true - np.mean(y_true))**2)\n    return float(1 - ss_res/ss_tot)\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "import numpy as np\nimport matplotlib.pyplot as plt\n\nX, y, feature_names = load_regression_data()\n(X_train, y_train), (X_val, y_val), (X_test, y_test) = train_val_test_split(X, y)\n\n# Add intercept\nXtr_i = np.c_[np.ones((X_train.shape[0], 1)), X_train]\nXval_i = np.c_[np.ones((X_val.shape[0], 1)), X_val]\n\ndef mse_grad(Xi, y, beta):\n    n = Xi.shape[0]\n    return (2.0/n) * (Xi.T @ (Xi @ beta - y))\n\ndef rmse(y_true, y_pred):\n    return float(np.sqrt(np.mean((y_true - y_pred)**2)))\n\n# TODO: Implement GD\ndef gd(Xi, y, lr=1e-2, iters=1000, beta0=None):\n    n, d = Xi.shape\n    beta = np.zeros(d) if beta0 is None else beta0.copy()\n    losses = []\n    for t in range(iters):\n        grad = mse_grad(Xi, y, beta)\n        beta -= lr * grad\n        losses.append(np.mean((Xi @ beta - y)**2))\n    return beta, np.array(losses)\n\n# TODO: Implement mini-batch SGD\ndef sgd(Xi, y, lr=1e-2, iters=2000, batch_size=64, beta0=None, seed=42):\n    rng = np.random.default_rng(seed)\n    n, d = Xi.shape\n    beta = np.zeros(d) if beta0 is None else beta0.copy()\n    losses = []\n    for t in range(iters):\n        idx = rng.choice(n, size=batch_size, replace=False)\n        Xi_b = Xi[idx]; y_b = y[idx]\n        grad = (2.0/batch_size) * (Xi_b.T @ (Xi_b @ beta - y_b))\n        beta -= lr * grad\n        # Track full-batch loss occasionally\n        if t % max(1, iters//200) == 0:\n            losses.append(np.mean((Xi @ beta - y)**2))\n    return beta, np.array(losses)\n\n# Run and compare\nbeta_gd, losses_gd = gd(Xtr_i, y_train, lr=1e-2, iters=800)\nbeta_sgd, losses_sgd = sgd(Xtr_i, y_train, lr=5e-3, iters=5000, batch_size=128)\n\nplt.figure()\nplt.plot(losses_gd, label=\"GD\")\nplt.plot(losses_sgd, label=\"SGD\")\nplt.title(\"Training loss vs steps\")\nplt.xlabel(\"Step\")\nplt.ylabel(\"MSE\")\nplt.legend()\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# TODO: Evaluate RMSE on val for both; try different lrs and batch sizes; document what works and why.\nyhat_gd = Xval_i @ beta_gd\nyhat_sgd = Xval_i @ beta_sgd\nprint(\"GD RMSE (val):\", rmse(y_val, yhat_gd))\nprint(\"SGD RMSE (val):\", rmse(y_val, yhat_sgd))\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}